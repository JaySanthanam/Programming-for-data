{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaySanthanam/Programming-for-data/blob/main/Worksheets/12_WatsonToneAnalyser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyS2Fb7jTftO"
      },
      "source": [
        "# Mini-project - creating a dataframe from analysed text data\n",
        "\n",
        "For this project you are going to use the IBM Watson Tone Analyser API.  You will send text data to it, use security information stored in a config file to keep it secret, receive the results in JSON format, investigate the structure of the results and build a dataframe from them.\n",
        "\n",
        "Then you will use the results to create a visualisation of tone and to report an overall set of statistics from the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsZ4_b2rTftU"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 1 - sign up for IBM Watson services to use the Tone Analyser\n",
        "\n",
        "1.  Sign up for [IBM Watson](https://cloud.ibm.com/registration?cm_mmc=dw-_-cognitive-_-topcoder-_-communityEducational1)\n",
        "2.  Click 'Try on Cloud at no cost'  \n",
        "3.  Select the London region  (costs reduced and performance improved when you use the nearest servers)  \n",
        "4.  Create an IBM Cloud account (enter email and accept terms)  \n",
        "5.  Follow the instructions to create the account  \n",
        "6.  Provision the services  \n",
        "7.  Then go to IBM Watson Studio  \n",
        "8.  Select Tone Analyzer under the Your Services heading  \n",
        "9.  You will be shown the **url** for the Tone Analyser API and an **API key** which is needed for using the API."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 - add security to your worksheet to keep your apikey and url hidden\n",
        "\n",
        "You can do this by using environment variables, which are stored in the operating system for this worksheet.\n",
        "\n",
        "We will use a simplified system for storing the sensitive data so that it isn't visible in the worksheet:\n",
        "\n",
        "1.  Ask for the api key to be input and store it in an environment variable called apikey\n",
        "\n",
        "2.  Ask for the url to be input and store it in an environment variable\n",
        "\n",
        "3.  Run the cell, type in the api key, then the url.  Once tis has been done.  Remove the output part of the cell."
      ],
      "metadata": {
        "id": "ZV19Vcgp2JHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# add the code to ask for the URL, then run this cell and when it has completed, remove the output (note: you will need to do this again if you return to the worksheet)\n",
        "os.environ['APIKEY'] = input(\"Enter API key: \")\n",
        "os.environ['URL'] = input(\"Enter URL: \")\n"
      ],
      "metadata": {
        "id": "ZM4PRCOZ2zyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install the ibm_watson libraries so that you can use their functions"
      ],
      "metadata": {
        "id": "fOKQWCOw1AGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the ibm_watson libraries\n",
        "\n",
        "!pip install ibm_watson"
      ],
      "metadata": {
        "id": "Uhi1cWMwQrZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae4ea46d-c7a4-4346-8c3d-309954ce4d0a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ibm_watson\n",
            "  Downloading ibm-watson-5.3.1.tar.gz (413 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 30 kB 23.0 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 40 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |████                            | 51 kB 16.7 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 61 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 71 kB 18.5 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 81 kB 19.1 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 92 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |████████                        | 102 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 112 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 122 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 133 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 143 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 153 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 163 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 174 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 184 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 194 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 204 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 215 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 225 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 235 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 245 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 256 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 266 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 276 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 286 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 296 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 307 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 317 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 327 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 337 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 348 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 358 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 368 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 378 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 389 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 399 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 409 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 413 kB 18.6 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ibm-cloud-sdk-core==3.*,>=3.3.6\n",
            "  Downloading ibm-cloud-sdk-core-3.14.0.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from ibm_watson) (2.8.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from ibm_watson) (2.23.0)\n",
            "Collecting websocket-client==1.1.0\n",
            "  Downloading websocket_client-1.1.0-py2.py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting requests<3.0,>=2.0\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting urllib3<2.0.0,>=1.26.0\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 54.2 MB/s \n",
            "\u001b[?25hCollecting PyJWT<3.0.0,>=2.0.1\n",
            "  Downloading PyJWT-2.3.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.5.3->ibm_watson) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.0->ibm_watson) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.0->ibm_watson) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.0->ibm_watson) (2.10)\n",
            "Building wheels for collected packages: ibm-watson, ibm-cloud-sdk-core\n",
            "  Building wheel for ibm-watson (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-watson: filename=ibm_watson-5.3.1-py3-none-any.whl size=409192 sha256=4678dc72bb009598abee540e47b9e967699f9d2cc223458e56547c5c263fb1dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/37/94/3d98f00e5be5dc05434c93028f36ae7ff06705f7939f04797b\n",
            "  Building wheel for ibm-cloud-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cloud-sdk-core: filename=ibm_cloud_sdk_core-3.14.0-py3-none-any.whl size=83275 sha256=d9d1a353c07a8f41f7a7875db1d062522ddc1a9fa5aaedd2a4c52d2b622c0bb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/d3/a0/fa4679c34222e203548311390d6baefe8c0e8fddadde1efa73\n",
            "Successfully built ibm-watson ibm-cloud-sdk-core\n",
            "Installing collected packages: urllib3, requests, PyJWT, websocket-client, ibm-cloud-sdk-core, ibm-watson\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed PyJWT-2.3.0 ibm-cloud-sdk-core-3.14.0 ibm-watson-5.3.1 requests-2.27.1 urllib3-1.26.8 websocket-client-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tqs2PZBXTftW"
      },
      "source": [
        "---\n",
        "\n",
        "## Test to make sure it works\n",
        "\n",
        "1.  Open this file, which has some text for you to test with: https://drive.google.com/file/d/1m65cPQGYQd1mwvEmfZw69-GMUBdo43k0/view?usp=sharing.  You will be able to copy and paste the text into here as needed.\n",
        "\n",
        "2.  Get the environment variable for each of the two pieces of security information so that these do not need to be included in your notebook (have the keys available for copying and pasting).  To do this:\n",
        "\n",
        "  ``` apikey = os.environ.get('APIKEY') ```\n",
        "\n",
        "3.  Run the code below,which will create a ToneAnalyzer with the credentials from your environment variables, then paste the text from the **text-for-analysis.txt** file\n",
        "\n",
        "4.  Decide what the data looks like and how this might be represented in a pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f_zyVYfTftY"
      },
      "outputs": [],
      "source": [
        "from ibm_watson import ToneAnalyzerV3\n",
        "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
        "import os\n",
        "import json\n",
        "\n",
        "# get credentials from the environment variables you set\n",
        "def get_secret(key):\n",
        "    # add code here to get the keys from the environment variable and return the requested key\n",
        "    # if there is an error print an error message and return None\n",
        "    return os.environ.get(key)\n",
        "\n",
        "\n",
        "    \n",
        "def get_text_for_analysis():\n",
        "    # add code here to input the text from the text-for-analysis.txt file and return the text it reads as one string\n",
        "    # if there is an error, return None\n",
        "    # return 'Team, I know that times are tough! Product sales have been disappointing for the past three quarters. We have a competitive product, but we need to do a better job of selling it!'\n",
        "    return 'But I feel peaceful. Your success in the ring this morning was, to a small degree, my success. Your future is assured. You will live, secure and safe, Wilbur. Nothing can harm you now. These autumn days will shorten and grow cold. The leaves will shake loose from the trees and fall. Christmas will come, and the snows of winter. You will live to enjoy the beauty of the frozen world, for you mean a great deal to Zuckerman and he will not harm you, ever. Winter will pass, the days will lengthen, the ice will melt in the pasture pond. The song sparrow will return and sing, the frogs will awake, the warm wind will blow again. All these sights and sounds and smells will be yours to enjoy, Wilbur-this lovely world, these precious days.'\n",
        "     \n",
        "    \n",
        "# create a ToneAnalyzerV3 object, version 2017-09-21 using api key and url from config\n",
        "authenticator = IAMAuthenticator(apikey=get_secret('APIKEY'))\n",
        "tone_analyzer = ToneAnalyzerV3(\n",
        "    version='2017-09-21',\n",
        "    authenticator=authenticator\n",
        ")\n",
        "tone_analyzer.set_service_url(get_secret('URL'))\n",
        "\n",
        "# get the text for analysis from the file\n",
        "text = get_text_for_analysis()\n",
        "if text:\n",
        "    tone_analysis = tone_analyzer.tone(\n",
        "        {'text': text},\n",
        "        content_type='application/json'\n",
        "    ).get_result()    \n",
        "    print(tone_analysis)\n",
        "else:\n",
        "    print(\"No data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WORKING WITH THE OUTPUT OF TEXT ANALYSIS"
      ],
      "metadata": {
        "id": "6faYfT24rI7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Watson Tone Analyser API Output\n",
        "---\n",
        "\n",
        "This is the output that the given text will produce. You will need to assign this output to a variable called **tone_analysis** in the function you are going to write below. \n",
        "\n",
        "```\n",
        "{'document_tone': {'tones': [{'score': 0.582191, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.829888, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]}, 'sentences_tone': [{'sentence_id': 0, 'text': 'Team, I know that times are tough!', 'tones': [{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]}, {'sentence_id': 1, 'text': 'Product sales have been disappointing for the past three quarters.', 'tones': [{'score': 0.817406, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]}, {'sentence_id': 2, 'text': 'We have a competitive product, but we need to do a better job of selling it!', 'tones': [{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]}]}\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "9hjqn6lrd4RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tone_analysis = {'document_tone': {'tones': [{'score': 0.582191, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.829888, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]}, 'sentences_tone': [{'sentence_id': 0, 'text': 'Team, I know that times are tough!', 'tones': [{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]}, {'sentence_id': 1, 'text': 'Product sales have been disappointing for the past three quarters.', 'tones': [{'score': 0.817406, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.687768, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]}, {'sentence_id': 2, 'text': 'We have a competitive product, but we need to do a better job of selling it!', 'tones': [{'score': 0.506763, 'tone_id': 'analytical', 'tone_name': 'Analytical'}]}]}"
      ],
      "metadata": {
        "id": "BCKdlM0ZAKDz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To me it looks like a nested set of dictionaries. This is the second time, I am seeing a JSON data output. Last time, we looked at retrieving JSON data from API we had clearly drawn out specific part of regional data marked \"data\". This time we have a whole output so we will have to learn how to separate all this into a format that can be easily understood. "
      ],
      "metadata": {
        "id": "rw3EVKJkPrHa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMHlV6wpTftb"
      },
      "source": [
        "### Create (on paper) an idea of how this data might be organised into a data table\n",
        "\n",
        "1.  How many bits of information are there about the document as a whole?\n",
        "2.  How many bits of information are there about each sentence?\n",
        "3.  If all tone analysis records were included in the dataframe, how many rows would there be?\n",
        "4.  What information would be included in each row?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. There are 2 document tones and 3 sentence tones, marked by sentence_id from 0-2. However, sentence id 1 has 2 scores associated with it.\n",
        "\n",
        "2. Each document has score, tone_id and tone_name. Each of the se0ntence has tones that have the following:\n",
        "*   text\n",
        "*   score\n",
        "*   tone_id\n",
        "*   tone_name\n",
        "\n",
        "3. There are 3 sentence rows (one for each id) and 2 rows for document tone. If we merge all of these, I would expect 6 rows( including 2 rows for sentence 1).\n",
        "\n",
        "4. In each row we will have the score, tone_id and tone_name.\n",
        "\n",
        "All of these are based on me working out on MS WORD by looking at the different levels of dictonaries from the out put. This is what I have from MS WORD\n",
        "\n",
        "{“document_tone”:\n",
        "{“tones”: [{“score”: 0.582191, ”tone_id”: ”sadness”, ”tone_name”: ”Sadness”}, \n",
        "{“score”: 0.829888, ”tone_id”: ”analytical”, ”tone_name”: ”Analytical”}\n",
        "\n",
        "“sentences_tone”: \n",
        "{“sentence_id”: 0, ”text”: ”Team, I know that times are tough!”, ”tones”: [{“score”: 0.801827, ”tone_id”: ”analytical”, ”tone_name”: ”Analytical”}]}, \n",
        "\n",
        "{“sentence_id”: 1, ”text”: ”Product sales have been disappointing for the past three quarters.”, \n",
        "”tones”: [{“score”: 0.817406, ”tone_id”: ”sadness”, ”tone_name”: ”Sadness”},\n",
        "{“score”: 0.687768, ”tone_id”: ”analytical”, ”tone_name”: ”Analytical”}]}, \n",
        "\n",
        "{“sentence_id”: 2, ”text”: ”We have a competitive product, but we need to do a better job of selling it!”, ”tones”: [{“score”: 0.506763, ”tone_id”: ”analytical”, ”tone_name”: ”Analytical”}]}\n",
        "\n",
        "As you can see, the second sentence has 2 possible tones. \n",
        "\n",
        "\n",
        "Based on this, I will try to work out the following questions."
      ],
      "metadata": {
        "id": "UOZRzdaeJo5N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ-p60NeTftc"
      },
      "source": [
        "### Create a dataframe and start to populate with the data\n",
        "\n",
        "You can create a **dataframe** from this data either by converting it manually into a table OR by using the pandas function pd.json_normalise(data).  \n",
        "\n",
        "**Manually**:\n",
        "\n",
        "One way to do this would be to create a list of dictionary records, with each record formed from the data from each row in the original 'sentences_tone' data.  You will need to loop through the rows in the 'sentences_tone' list, nesting a loop through the 'tones' list for each sentence.  For each, copy across the columns you feel should be included.\n",
        "\n",
        "_Hint:_  \n",
        "```\n",
        " for row in sentence_data:\n",
        "        for col in row['tones']:\n",
        "            new_row = {'sentence_id':row['sentence_id'], 'text':row['text'], 'tone_score':col['score'], 'tone_id':col['tone_id'],'tone_name':col['tone_name']}\n",
        "```\n",
        "**Using pandas**:\n",
        "\n",
        "An alternative way to do this would be to create a pandas dataframe from the sentences_tone data list (using `pd.json_normalise(data)`)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I would prefer to use pandas library to do this - only because it will save time from reinvening the wheel and also will allow be to practice the things I learned to do with pandas once more."
      ],
      "metadata": {
        "id": "1R4UncJaRUzY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "thFgfS13Tftc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82efea65-f292-40e8-8818-64db4a2cb4f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The document tones are\n",
            "      score     tone_id   tone_name\n",
            "0  0.582191     sadness     Sadness\n",
            "1  0.829888  analytical  Analytical\n",
            "The sentences' tones are\n",
            "      score  ...                                               text\n",
            "0  0.801827  ...                 Team, I know that times are tough!\n",
            "1  0.817406  ...  Product sales have been disappointing for the ...\n",
            "2  0.687768  ...  Product sales have been disappointing for the ...\n",
            "3  0.506763  ...  We have a competitive product, but we need to ...\n",
            "\n",
            "[4 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# convert json data to a dataframe with one row for each tone for each sentence\n",
        "def convert_to_tones_table(data):\n",
        "    # return the data normalized into a dataframe (pd.json_normalise(data))\n",
        "    # the dataframe should have the columns: sentence_id, text, score, tone_id, tone_name\n",
        "    df = pd.json_normalize(data)\n",
        "    return df\n",
        "\n",
        "#split the data into document data and sentence data\n",
        "tone_analysis = {\"document_tone\": {\"tones\": [{\"score\": 0.582191, \"tone_id\": \"sadness\", \"tone_name\": \"Sadness\"}, {\"score\": 0.829888, \"tone_id\": \"analytical\", \"tone_name\": \"Analytical\"}]}, \"sentences_tone\": [{\"sentence_id\": 0, \"text\": \"Team, I know that times are tough!\", \"tones\": [{\"score\": 0.801827, \"tone_id\": \"analytical\", \"tone_name\": \"Analytical\"}]}, {\"sentence_id\": 1, \"text\": \"Product sales have been disappointing for the past three quarters.\", \"tones\": [{\"score\": 0.817406, \"tone_id\": \"sadness\", \"tone_name\": \"Sadness\"}, {\"score\": 0.687768, \"tone_id\": \"analytical\", \"tone_name\": \"Analytical\"}]}, {\"sentence_id\": 2, \"text\": \"We have a competitive product, but we need to do a better job of selling it!\", \"tones\": [{\"score\": 0.506763, \"tone_id\": \"analytical\", \"tone_name\": \"Analytical\"}]}]}\n",
        "doc_data = convert_to_tones_table(tone_analysis[\"document_tone\"])\n",
        "sent_data = convert_to_tones_table(tone_analysis[\"sentences_tone\"])\n",
        "\n",
        "#print(doc_data)\n",
        "#print(sent_data)\n",
        "\n",
        "#Further go through the nested dictionaries and separate the various levels of document data\n",
        "def doc_analyses(data):\n",
        "  doc_df = convert_to_tones_table(data[\"tones\"])\n",
        "  piv_doc_df = doc_df.transpose()\n",
        "  doc_final = piv_doc_df[0].apply(pd.Series)\n",
        "  return doc_final\n",
        "\n",
        "doc_final = doc_analyses(doc_data)\n",
        "\n",
        "#Further go through the nested dictionaries and separate the various levels of sentence data\n",
        "def sent_analyses(data):\n",
        "  sent_data = data.explode([\"tones\"])\n",
        "  sent_df = convert_to_tones_table(sent_data[\"tones\"])\n",
        "  sent_final = sent_df\n",
        "  sent_final['text'] = sent_data['text'].values #This adds back relevant text to the tones\n",
        "  return sent_final\n",
        "\n",
        "sent_final = sent_analyses(sent_data)\n",
        "\n",
        "print(\"The document tones are\")\n",
        "print(doc_final)\n",
        "print(\"The sentences' tones are\")\n",
        "print(sent_final)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I now have two dataframes: one for document tones and another for sentence tones. We can do the usual analyses of these data frames using pandas as required. This was very interested worksheet. I am left with the thought that I may have to know more of the JSON data looks like before hand so I know how to handle it, how to normalize it and how many iteretaions of normalizations will be required (which entirely depends on how many levels of informations there are!). From a programming point of view, I have looked at how to data retrieval and wrangling here."
      ],
      "metadata": {
        "id": "CrEm_DuZSNR1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LBNPm67Tfte"
      },
      "source": [
        "### Summarise the sentence data\n",
        "*  Which sentence is the most analytical?\n",
        "*  which sentence is the least analytical?\n",
        "*  what is the average analytical tone score for the sentences?\n",
        "*  what do the analytical scores look like in a bar chart?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "e9QNeA6gTftf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c416d5-055e-4395-887c-109cd0283c62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentence with maximum analytical score is:  0    Team, I know that times are tough!\n",
            "Name: text, dtype: object\n",
            "The sentence with minimum analytical score is:  3    We have a competitive product, but we need to ...\n",
            "Name: text, dtype: object\n",
            "The average analytical score is:  0.6655\n"
          ]
        }
      ],
      "source": [
        "def tone_filter(str, df):\n",
        "  is_str = df['tone_name']== str\n",
        "  df = df[is_str]\n",
        "  return df\n",
        "\n",
        "\n",
        "def get_tone_stats(df):\n",
        "  max_sent = df.loc[df['score']==df['score'].max(),'text']\n",
        "  min_sent = df.loc[df['score']==df['score'].min(),'text']\n",
        "  ave_tone = df['score'].mean()\n",
        "  return max_sent, min_sent, ave_tone\n",
        "\n",
        "sent_analy = tone_filter(\"Analytical\", sent_final)\n",
        "\n",
        "\n",
        "analy_tone_stats = get_tone_stats(sent_analy)\n",
        "print(\"The sentence with maximum analytical score is: \", analy_tone_stats[0])\n",
        "print(\"The sentence with minimum analytical score is: \", analy_tone_stats[1])\n",
        "print(\"The average analytical score is: \", \"{:.4f}\".format(analy_tone_stats[2]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have everything into dataframes, this question was smooth sailing, again, it gave a an opportunity to practice sorting, filtering and get useful stats from the dataframe. I have put the stats calculation into a function, since in reality we may want to repeat these operrations for other senetence tones. In the current scenario, there is just one senetence with sadness tone, so it doesn't make sense to look at the aggregates for that. However, I have done the function as a practice."
      ],
      "metadata": {
        "id": "7C1tU9EWS8bS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's build a bar chart of the scores with pandas! I think I have used matplotlib and seaborn a lot. I want to practice a bit with using pandas to plot."
      ],
      "metadata": {
        "id": "d-jaokzHelSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_colors = list(['b', 'r', 'g', 'k'])\n",
        "\n",
        "ax = sent_final.plot.barh(x='tone_name', y='score', rot=0, color=my_colors)\n",
        "ax.set_xlabel(\"Score\")\n",
        "ax.set_ylabel(\"Sentence tone\")"
      ],
      "metadata": {
        "id": "bE8H6xbWe6HV",
        "outputId": "d1f96c26-7361-4da7-eeda-15c9570abc7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Sentence tone')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5QvI5ilTftf"
      },
      "source": [
        "### Challenging:  Report the tone data for the whole document\n",
        "---\n",
        "\n",
        "Play with the data, create a dataframe for the document_tone, tones data\n",
        "\n",
        " ```pd.json_normalize(document_tone)```  \n",
        "\n",
        "Display the document score for each of the tones in the analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I already have a dataframe for document tone. So I will try to work with that to answer these questions."
      ],
      "metadata": {
        "id": "m0z2iezST1l1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "scrolled": true,
        "id": "ZY8_msgUTftg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e136b0-09da-4c3a-9e36-f5f3bfbeab0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      score     tone_id   tone_name\n",
            "0  0.582191     sadness     Sadness\n",
            "1  0.829888  analytical  Analytical\n"
          ]
        }
      ],
      "source": [
        "print(doc_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two tones in the document - sadness and analytical with scores of 0.5822 and 0.83 scores respectively."
      ],
      "metadata": {
        "id": "x0ScWMz7UB8K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0KZ9z-QTfth"
      },
      "source": [
        "### Change the text in the text file and analyse the new text.\n",
        "---\n",
        "\n",
        "Here is some alternative, happier text.  Replace the text in the text-for-analysis.txt file with the text below.  Then run the notebook cells again to see the results.\n",
        "\n",
        "But I feel peaceful. Your success in the ring this morning was, to a small degree, my success. Your future is assured. You will live, secure and safe, Wilbur. Nothing can harm you now. These autumn days will shorten and grow cold. The leaves will shake loose from the trees and fall. Christmas will come, and the snows of winter. You will live to enjoy the beauty of the frozen world, for you mean a great deal to Zuckerman and he will not harm you, ever. Winter will pass, the days will lengthen, the ice will melt in the pasture pond. The song sparrow will return and sing, the frogs will awake, the warm wind will blow again. All these sights and sounds and smells will be yours to enjoy, Wilbur-this lovely world, these precious days.\n",
        "\n",
        "### The result of this analysis is below:\n",
        "\n",
        "```\n",
        "{'document_tone': {'tones': [{'score': 0.525587, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.670614, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.802229, 'tone_id': 'confident', 'tone_name': 'Confident'}]}, 'sentences_tone': [{'sentence_id': 0, 'text': 'But I feel peaceful.', 'tones': [{'score': 0.511185, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]}, {'sentence_id': 1, 'text': 'Your success in the ring this morning was, to a small degree, my success.', 'tones': [{'score': 0.919911, 'tone_id': 'joy', 'tone_name': 'Joy'}]}, {'sentence_id': 2, 'text': 'Your future is assured.', 'tones': [{'score': 0.97759, 'tone_id': 'confident', 'tone_name': 'Confident'}]}, {'sentence_id': 3, 'text': 'You will live, secure and safe, Wilbur.', 'tones': [{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.92125, 'tone_id': 'confident', 'tone_name': 'Confident'}]}, {'sentence_id': 4, 'text': 'Nothing can harm you now.', 'tones': []}, {'sentence_id': 5, 'text': 'These autumn days will shorten and grow cold.', 'tones': []}, {'sentence_id': 6, 'text': 'The leaves will shake loose from the trees and fall.', 'tones': [{'score': 0.621679, 'tone_id': 'fear', 'tone_name': 'Fear'}]}, {'sentence_id': 7, 'text': 'Christmas will come, and the snows of winter.', 'tones': [{'score': 0.614764, 'tone_id': 'joy', 'tone_name': 'Joy'}]}, {'sentence_id': 8, 'text': 'You will live to enjoy the beauty of the frozen world, for you mean a great deal to Zuckerman and he will not harm you, ever.', 'tones': [{'score': 0.930779, 'tone_id': 'joy', 'tone_name': 'Joy'}]}, {'sentence_id': 9, 'text': 'Winter will pass, the days will lengthen, the ice will melt in the pasture pond.', 'tones': [{'score': 0.654012, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]}, {'sentence_id': 10, 'text': 'The song sparrow will return and sing, the frogs will awake, the warm wind will blow again.', 'tones': [{'score': 0.600542, 'tone_id': 'joy', 'tone_name': 'Joy'}]}, {'sentence_id': 11, 'text': 'All these sights and sounds and smells will be yours to enjoy, Wilbur-this lovely world, these precious days.', 'tones': [{'score': 0.939404, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.660207, 'tone_id': 'confident', 'tone_name': 'Confident'}]}]}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a new set of document, sentences and their tones. Looking into the results, I see 2 document tones and 12 sentences. As with the previous case, some of the sentences have more than 1 tone. I am going to use the same function as before to analyse this result as well and convert them into two dataframes - tone_doc and tone_sent respectively.\n",
        "\n",
        "**Note: I did not want to replace my previous variable. I am aware that it takes up memeory. In some case, it does make sense to replace the variable and run the code multiple times. But this time, I want to keep the previous work untouched so I can look at it later for practice. So I have created a new variable, tone_to_analyse to start the analyses."
      ],
      "metadata": {
        "id": "D9Aui2m_U2Iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tone_to_analyse = {'document_tone': {'tones': [{'score': 0.525587, 'tone_id': 'sadness', 'tone_name': 'Sadness'}, {'score': 0.670614, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.802229, 'tone_id': 'confident', 'tone_name': 'Confident'}]}, 'sentences_tone': [{'sentence_id': 0, 'text': 'But I feel peaceful.', 'tones': [{'score': 0.511185, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.88939, 'tone_id': 'tentative', 'tone_name': 'Tentative'}]}, {'sentence_id': 1, 'text': 'Your success in the ring this morning was, to a small degree, my success.', 'tones': [{'score': 0.919911, 'tone_id': 'joy', 'tone_name': 'Joy'}]}, {'sentence_id': 2, 'text': 'Your future is assured.', 'tones': [{'score': 0.97759, 'tone_id': 'confident', 'tone_name': 'Confident'}]}, {'sentence_id': 3, 'text': 'You will live, secure and safe, Wilbur.', 'tones': [{'score': 0.801827, 'tone_id': 'analytical', 'tone_name': 'Analytical'}, {'score': 0.92125, 'tone_id': 'confident', 'tone_name': 'Confident'}]}, {'sentence_id': 4, 'text': 'Nothing can harm you now.', 'tones': []}, {'sentence_id': 5, 'text': 'These autumn days will shorten and grow cold.', 'tones': []}, {'sentence_id': 6, 'text': 'The leaves will shake loose from the trees and fall.', 'tones': [{'score': 0.621679, 'tone_id': 'fear', 'tone_name': 'Fear'}]}, {'sentence_id': 7, 'text': 'Christmas will come, and the snows of winter.', 'tones': [{'score': 0.614764, 'tone_id': 'joy', 'tone_name': 'Joy'}]}, {'sentence_id': 8, 'text': 'You will live to enjoy the beauty of the frozen world, for you mean a great deal to Zuckerman and he will not harm you, ever.', 'tones': [{'score': 0.930779, 'tone_id': 'joy', 'tone_name': 'Joy'}]}, {'sentence_id': 9, 'text': 'Winter will pass, the days will lengthen, the ice will melt in the pasture pond.', 'tones': [{'score': 0.654012, 'tone_id': 'sadness', 'tone_name': 'Sadness'}]}, {'sentence_id': 10, 'text': 'The song sparrow will return and sing, the frogs will awake, the warm wind will blow again.', 'tones': [{'score': 0.600542, 'tone_id': 'joy', 'tone_name': 'Joy'}]}, {'sentence_id': 11, 'text': 'All these sights and sounds and smells will be yours to enjoy, Wilbur-this lovely world, these precious days.', 'tones': [{'score': 0.939404, 'tone_id': 'joy', 'tone_name': 'Joy'}, {'score': 0.660207, 'tone_id': 'confident', 'tone_name': 'Confident'}]}]}\n",
        "\n",
        "\n",
        "document = convert_to_tones_table(tone_to_analyse[\"document_tone\"])\n",
        "sentence = convert_to_tones_table(tone_to_analyse[\"sentences_tone\"])\n",
        "\n",
        "tone_doc = doc_analyses(document)\n",
        "\n",
        "tone_sent = sent_analyses(sentence)\n",
        "\n",
        "print(tone_doc)\n",
        "\n",
        "print(tone_sent)"
      ],
      "metadata": {
        "id": "PJdmm1CmWZ4c",
        "outputId": "84877383-484b-4c3c-9d00-0ef8de24a08a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      score    tone_id  tone_name\n",
            "0  0.525587    sadness    Sadness\n",
            "1  0.670614        joy        Joy\n",
            "2  0.802229  confident  Confident\n",
            "       score  ...                                               text\n",
            "0   0.511185  ...                               But I feel peaceful.\n",
            "1   0.889390  ...                               But I feel peaceful.\n",
            "2   0.919911  ...  Your success in the ring this morning was, to ...\n",
            "3   0.977590  ...                            Your future is assured.\n",
            "4   0.801827  ...            You will live, secure and safe, Wilbur.\n",
            "5   0.921250  ...            You will live, secure and safe, Wilbur.\n",
            "6        NaN  ...                          Nothing can harm you now.\n",
            "7        NaN  ...      These autumn days will shorten and grow cold.\n",
            "8   0.621679  ...  The leaves will shake loose from the trees and...\n",
            "9   0.614764  ...      Christmas will come, and the snows of winter.\n",
            "10  0.930779  ...  You will live to enjoy the beauty of the froze...\n",
            "11  0.654012  ...  Winter will pass, the days will lengthen, the ...\n",
            "12  0.600542  ...  The song sparrow will return and sing, the fro...\n",
            "13  0.939404  ...  All these sights and sounds and smells will be...\n",
            "14  0.660207  ...  All these sights and sounds and smells will be...\n",
            "\n",
            "[15 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let me try if I can filter the tones in these dataframes and see what I get. I didn't do it for the documnet tones in the previous question since it only had one score for each tone, so, further analyses makes no sense. It is the same here too. There are 3 tones in the document each with just one score. So further analyses and filtering of document tones is not useful. I will continue to the analyses of sentences though."
      ],
      "metadata": {
        "id": "DsdQ3bQ3a-YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#filter out various types of tones in the sentences dataframe\n",
        "sentence_analy = tone_filter(\"Analytical\", tone_sent)\n",
        "\n",
        "sentence_joy = tone_filter(\"Joy\", tone_sent)\n",
        "\n",
        "sentence_sad = tone_filter(\"Sadness\", tone_sent)\n",
        "\n",
        "sentence_conf = tone_filter(\"Confident\", tone_sent)\n",
        "\n",
        "sentence_tent = tone_filter(\"Tentative\", tone_sent)\n",
        "\n",
        "sentence_fear = tone_filter(\"Fear\", tone_sent)\n",
        "\n",
        "# Find out how many sentences in each of the categories\n",
        "\n",
        "analy_sent = sentence_analy.shape[0]\n",
        "joy_sent = sentence_joy.shape[0]\n",
        "sad_sent = sentence_sad.shape[0]\n",
        "conf_sent = sentence_conf.shape[0]\n",
        "fear_sent = sentence_fear.shape[0]\n",
        "tent_sent = sentence_tent.shape[0]\n",
        "\n",
        "print(\"No. of sentences that have Analytical tones: \", analy_sent)\n",
        "print(\"No. of sentences that have Joyful tones: \", joy_sent)\n",
        "print(\"No. of sentences that have sad tones: \", sad_sent)\n",
        "print(\"No. of sentences that have confident tones: \", conf_sent)\n",
        "print(\"No. of sentences that have fearful tones: \", fear_sent)\n",
        "print(\"No. of sentences that have tentative tones: \", tent_sent)\n"
      ],
      "metadata": {
        "id": "vo_jb1W1b-g6",
        "outputId": "30c2915f-47ce-4d35-f8c3-724dd8cd8d97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of sentences that have Analytical tones:  1\n",
            "No. of sentences that have Joyful tones:  6\n",
            "No. of sentences that have sad tones:  1\n",
            "No. of sentences that have confident tones:  3\n",
            "No. of sentences that have fearful tones:  1\n",
            "No. of sentences that have tentative tones:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above tells gives me an idea of how the tones are distributed in the sentences."
      ],
      "metadata": {
        "id": "VXEFOA7ZebY7"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "12. WatsonToneAnalyser.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sea-Level-Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaySanthanam/Programming-for-data/blob/main/Worksheets/Sea_Level_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using numpy to look for a correlation between time data and sea level rise\n",
        "---\n",
        "\n",
        "### Data Source\n",
        "Global Average Absolute Sea Level Change, 1880-2014 from the US Environmental Protection Agency using data from CSIRO, 2015; NOAA, 2015.\n",
        "https://datahub.io/core/sea-level-rise\n",
        "\n",
        "The data describes annual sea levels from 1880 to 2013.  Measures are adjusted using two standards: Commonwealth Scientific and Industrial Research Organisation(CSIRO) and National Oceanic and Atmospheric Administration (NOAA)  \n",
        "\n",
        "Raw Data file:  https://raw.githubusercontent.com/freeCodeCamp/boilerplate-sea-level-predictor/master/epa-sea-level.csv\n",
        "\n",
        "For this exercise:\n",
        "*  import the pandas library\n",
        "*  import the numpy library\n",
        "*  read the csv dataset containing data on sea-levels from the year 1880 to 2013 into a dataframe (df)\n",
        "*  use df.head() and df.info() to inspect the data and the column data types\n",
        "\n"
      ],
      "metadata": {
        "id": "jBYNdCdQ9_cu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Note 1\n",
        "This part is straight forward. I always prefer keeping the url in the main code and leave the function to simply read from the url. This is just in case the url gets changed or if I decide to use the same function but to read another data set from a different url. I feel this allows more flexibility."
      ],
      "metadata": {
        "id": "PgAqunyrsqgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_data(url): # this function gets data stored as csv file from a given url\n",
        "  df = pd.read_csv(url)\n",
        "  return df\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/freeCodeCamp/boilerplate-sea-level-predictor/master/epa-sea-level.csv\"\n",
        "sea_level = get_data(url)\n",
        "print(sea_level.head()) #first 5 rows\n",
        "print(sea_level.info()) #technical summary"
      ],
      "metadata": {
        "id": "r1XUCWHV_Cj9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4561e6-3207-4eac-9010-e5b65e9f0cf1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Year  CSIRO Adjusted Sea Level  ...  Upper Error Bound  NOAA Adjusted Sea Level\n",
            "0  1880                  0.000000  ...           0.952756                      NaN\n",
            "1  1881                  0.220472  ...           1.173228                      NaN\n",
            "2  1882                 -0.440945  ...           0.464567                      NaN\n",
            "3  1883                 -0.232283  ...           0.665354                      NaN\n",
            "4  1884                  0.590551  ...           1.464567                      NaN\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 134 entries, 0 to 133\n",
            "Data columns (total 5 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   Year                      134 non-null    int64  \n",
            " 1   CSIRO Adjusted Sea Level  134 non-null    float64\n",
            " 2   Lower Error Bound         134 non-null    float64\n",
            " 3   Upper Error Bound         134 non-null    float64\n",
            " 4   NOAA Adjusted Sea Level   21 non-null     float64\n",
            "dtypes: float64(4), int64(1)\n",
            "memory usage: 5.4 KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Then\n",
        "---\n",
        "1.  Calculate some statistics on the level array, eg:\n",
        "*  mean\n",
        "*  standard deviation\n",
        "*  total \n",
        "\n",
        "2.  Use the fact that the arrays are aligned (e.g. the first number in the level array is linked to the first year in the year array and display:\n",
        "\n",
        "*  the year with the biggest rise in level\n",
        "*  the year with the lowest rise in level\n",
        "\n",
        "*(**Hint**:  to do this you can use a new numpy function np.where() )*\n",
        " ```\n",
        "np.where(array == value_to_find)\n",
        "```\n",
        "*There is some reference material [here](https://thispointer.com/find-the-index-of-a-value-in-numpy-array/)*\n",
        "\n",
        "**Note**: ```np.where(...)``` will return a tuple containing all indexes where that value was found.  You can print all, or you can print the first value (it is likely that there will only be one in this case) using [0][0].  *With the correct code you should get an answer of 2012*\n",
        "\n",
        "\n",
        "3.  Calculate the Pearson product-moment correlation coefficient between year and the rise in sea level.  (*Expected output:  0.98 when rounded to 2 decimal places*)"
      ],
      "metadata": {
        "id": "3cf1YPgnBSc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note 2:\n",
        "Before proceeding any further, I want to get rid of null values to make life easier for me. I got the following when I tested for null values (code is now commented since it is unnecessary).\n",
        "\n",
        "\n",
        "Year                        False\n",
        "\n",
        "CSIRO Adjusted Sea Level    False\n",
        "\n",
        "Lower Error Bound           False\n",
        "\n",
        "Upper Error Bound           False\n",
        "\n",
        "NOAA Adjusted Sea Level      True\n",
        "\n",
        "dtype: bool\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "All columns except NOAA adjusted sea level have no null values. So I will drop that off to make a new data frame. I'm aware that this means that I am missing out on some of the information. But for now, I will work with the CSIRO Adjusted Sea Level. This exact calculations can be repeated with the NOAA adjusted sea level column as well. So I am saving the dataframe after dropping the NOAA column into a new dataframe."
      ],
      "metadata": {
        "id": "38E2hmFE-G3H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8VWUtmK4YhoU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74379943-92d8-4c69-fe70-9975d51c228b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average sea level is 3.650340811880598 \n",
            "The standard deviation of sea_level is 2.476399631426498 \n",
            "The total sea level is 489.1456687920001\n",
            "The sea level was highest at 9.326771643999999 in the year [2012]\n",
            "The sea level was lowest at -0.440944881 in the year [1882]\n"
          ]
        }
      ],
      "source": [
        "#find any null values\n",
        "#def any_nulls(df):\n",
        "#  df_null = df.isna().any()\n",
        "#  return df_null\n",
        "#null_values = any_nulls(sea_level)\n",
        "#print(null_values)\n",
        "\n",
        "df_new = sea_level.drop(columns=['NOAA Adjusted Sea Level'])\n",
        "sea_level_arr = np.array(df_new[\"CSIRO Adjusted Sea Level\"],np.float64)\n",
        "sea_year_arr = np.array(df_new[\"Year\"],np.int64)\n",
        "sea_mean = np.mean(sea_level_arr)\n",
        "sea_std = np.std(sea_level_arr)\n",
        "sea_total = np.sum(sea_level_arr)\n",
        "print(\"The average sea level is\", sea_mean, \"\\nThe standard deviation of sea_level is\", sea_std, \"\\nThe total sea level is\", sea_total)\n",
        "\n",
        "max_level = np.amax(sea_level_arr)\n",
        "min_level = np.amin(sea_level_arr)\n",
        "max_index = np.where(sea_level_arr == max_level)\n",
        "max_year = max_index[0] +sea_year_arr[0]\n",
        "min_index = np.where(sea_level_arr == min_level)\n",
        "min_year = min_index[0] +sea_year_arr[0]\n",
        "print(\"The sea level was highest at\", max_level, \"in the year\", max_year)\n",
        "print(\"The sea level was lowest at\", min_level, \"in the year\", min_year)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Note 3:\n",
        "Next I want to find the Pearson product-moment correlation coefficient between year and the rise in sea level. I'm aware that we are missing NOAA adjusted sea level. We can also repeat the same for that column with just the 21 rows of available data. But for now, I'm going to continue working with CSIRO Adjusted Sea Level for all years from 1880."
      ],
      "metadata": {
        "id": "5xJje1bcpM-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_correlation(nparray1,nparray2):\n",
        "  # code to get the correlation figure for both salaries\n",
        "  R_matrix = np.corrcoef(nparray1, nparray2)\n",
        "  return R_matrix\n",
        "\n",
        "R_value = get_correlation(sea_level_arr,sea_year_arr)\n",
        "print(\"The Pearson correlation coefficient between year and sea level is:\"\"{:10.2f}\".format(R_value[0][1]))\n",
        "print(\"This shows that there is a strong correlation between year and rising sea levels\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOVrxtlLqYHD",
        "outputId": "7372523e-aca5-45de-fa20-90e72aca9a49"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Pearson correlation coefficient between year and sea level is:      0.98\n",
            "This shows that there is a strong correlation between year and rising sea levels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQyytEbnZ1lw"
      },
      "source": [
        "# Reflection\n",
        "----\n",
        "\n",
        "## What skills have you demonstrated in completing this notebook?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM00hR5aZk1-"
      },
      "source": [
        "Your answer: I have used my data retrieval, cleaning and wrangling skills and have been able to use pandas and numpy libraries for this project. I used pandas library for retrieving, sorting and cleaning the data while I used numpy to do Statistical analyses. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgexd27sZ1ly"
      },
      "source": [
        "## What caused you the most difficulty?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y_nrVBwaGXr"
      },
      "source": [
        "Your answer: This was a fairly easy workbook. I think numpy is quite handy when we have large amount of numeric data. The use of arrays and matrices can go beyond use in Statistics and I am keen to know more about using numpy for calculations involving matrices."
      ]
    }
  ]
}